{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, sys, os\n",
    "import json, torch, random\n",
    "import numpy as np\n",
    "sys.path.append(\"../\")\n",
    "import utilities as utl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pympler import asizeof\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from bkmeans import BKMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import div_utilities as div_utl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_barchart(dataframe, algorithm_col, time_col, query_col, annotate_values = False, annotate_query_tables = False):\n",
    "    \"\"\"\n",
    "    Plot a grouped bar chart with queries on the X-axis, time on the Y-axis, and each group containing bars for different algorithms.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: Pandas DataFrame containing the data.\n",
    "    - algorithm_col: Name of the column containing algorithm names.\n",
    "    - time_col: Name of the column containing time values.\n",
    "    - query_col: Name of the column containing query names.\n",
    "    \"\"\"\n",
    "    predefined_order = [r'$GMC$', r'$GNE$',r'$CLT$',r'$DUST$']\n",
    "    \n",
    "    # Get unique algorithms and queries\n",
    "    unique_algorithms = dataframe[algorithm_col].unique()\n",
    "    # print(unique_algorithms)\n",
    "    unique_queries = dataframe[query_col].unique()\n",
    "    sorted_indices = np.argsort(unique_queries)\n",
    "    unique_queries = unique_queries[sorted_indices]\n",
    "    # unique_algorithms = sorted(unique_algorithms, key=lambda x: predefined_order.index(x) if x in predefined_order else float('inf'))\n",
    "\n",
    "    # Set up positions for bars on X-axis\n",
    "    bar_width = 0.22 # Adjust the width as needed\n",
    "    # space_between_groups = 0.5\n",
    "    bar_positions = np.arange(len(unique_queries))\n",
    "\n",
    "    # # Define a color blindness-friendly palette with hatch patterns\n",
    "    #palette = ['#1f78b4', '#33a02c', '#e31a1c', '#ff7f00', '#6a3d9a', '#a6cee3', '#b2df8a', '#fb9a99']\n",
    "\n",
    "    # Hatch patterns for bars\n",
    "    #hatch_patterns = ['/', '\\\\', '|', '-', '+', 'x', 'o', '*']\n",
    "    \n",
    "    if len(unique_queries) == 3:\n",
    "        palette = ['#1f78b4', '#e31a1c', '#ff7f00']\n",
    "        # Hatch patterns for bars\n",
    "        hatch_patterns = ['/', '+', 'o']\n",
    "    else:\n",
    "        # Define a color blindness-friendly palette with hatch patterns\n",
    "        palette = ['#1f78b4', '#33a02c', '#e31a1c', '#ff7f00', '#6a3d9a', '#a6cee3']\n",
    "        hatch_patterns = ['/', '\\\\', '+', 'o', \"*\", \"-\"]\n",
    "\n",
    "    # Create a grouped bar chart\n",
    "    plt.figure(figsize=(19, 6))  # Adjust the figure size as needed\n",
    "\n",
    "    for i, algorithm in enumerate(unique_algorithms):\n",
    "        algorithm_data = dataframe[dataframe[algorithm_col] == algorithm]\n",
    "        mean_times = [algorithm_data[algorithm_data[query_col] == query][time_col].mean() for query in unique_queries]\n",
    "        print(\"Algorithm time: \", mean_times)\n",
    "        hatch_pattern = hatch_patterns[i % len(hatch_patterns)]\n",
    "        plt.bar(\n",
    "            bar_positions + i * (bar_width),\n",
    "            mean_times,\n",
    "            width=bar_width,\n",
    "            label=algorithm,\n",
    "            color=palette[i],\n",
    "            hatch=hatch_pattern,\n",
    "            edgecolor='black'  # Add black borders for better visibility\n",
    "        )\n",
    "    if annotate_values == True:\n",
    "        # Annotate each bar in the group\n",
    "        for i, query in enumerate(unique_queries):\n",
    "            for j, algorithm in enumerate(unique_algorithms):\n",
    "                mean_time = dataframe[(dataframe[algorithm_col] == algorithm) & (dataframe[query_col] == query)][time_col].mean()\n",
    "                plt.text(bar_positions[i] + j * bar_width, mean_time + 0.02, f'{round(mean_time)}', ha='center', va='bottom')\n",
    "    # Add labels and title\n",
    "    \n",
    "        # Set X-axis ticks and labels\n",
    "    # plt.xticks(bar_positions + (len(unique_algorithms) - 1) * bar_width / 2, unique_queries)\n",
    "    plt.xlim(bar_positions[0] - bar_width / 2 - 0.3, bar_positions[-1] + (len(unique_algorithms) - 0.5) * bar_width + 0.3)\n",
    "\n",
    "    if annotate_query_tables:\n",
    "        # Add labels below each group of bars\n",
    "        for i, query in enumerate(unique_queries):\n",
    "            plt.text((bar_positions + (len(unique_algorithms) - 1) * bar_width / 2)[i], -0.00012,\n",
    "                    f'{query[:15].strip()}...', rotation=70, ha='right', va='top')\n",
    "    # Add legend\n",
    "    plt.legend() #(loc = \"upper right\")\n",
    "    return plt\n",
    "        # # Show the plot\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_name = 'labeled_benchmark'\n",
    "folder_path = r'final_stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starmie_df = pd.read_csv(f\"{folder_path}/{benchmark_name}__cosine__dust__filtered_file.csv\")\n",
    "# llm_df = pd.read_csv(f\"{folder_path}/{benchmark_name}__cosine__dust.csv\")\n",
    "dust_df = pd.read_csv(f\"{folder_path}/{benchmark_name}__cosine__dust__our.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe = starmie_df\n",
    "\n",
    "# result_dataframe = pd.concat([result_dataframe, llm_df])\n",
    "result_dataframe = pd.concat([result_dataframe, dust_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe['algorithm'] = result_dataframe['algorithm'].replace({'gmc': r'$GMC$', 'gne': r'$GNE$', \"clt\": r'$CLT$', \"our\": r'$DUST$', \"our_base\": r'$DUST_{c}$', \"Starmie\": r'$Starmie$', \"LLM\": r\"$LLM$\"})\n",
    "exclude_algs = {r\"$DUST_{c}$\"} #, r'$GMC$', r'$CLT$',r'$GNE$'}\n",
    "distance_function = \"cosine\"\n",
    "with_query_flag = \"mix\"\n",
    "metric = \"max-min_div_score\" \n",
    "# metric = \"avg_div_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(result_dataframe['algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns_df = result_dataframe[['algorithm', 'query_name', 'time_taken_(s)']]\n",
    "# # Drop duplicates, if any\n",
    "# selected_columns_df['query_name'] = \"All Labeled Queries\" \n",
    "# selected_columns_df = selected_columns_df.drop_duplicates()\n",
    "# # Display the resulting DataFrame\n",
    "# print(selected_columns_df)\n",
    "# plt = plot_grouped_barchart(selected_columns_df, 'algorithm', 'time_taken_(s)', 'query_name', annotate_values=True)\n",
    "# #plt.ylim(0, 50, 5)\n",
    "# #plt.yscale(\"log\")\n",
    "# plt.xlabel('Query')\n",
    "# plt.ylabel('Time Taken (s)')\n",
    "# plt.title('Time Taken by Algorithm for Different Queries')\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where 'with_query_flag' is 'yes' and 'evaluation_distance_function' is 'cosine'\n",
    "\n",
    "subset_df = result_dataframe[(result_dataframe['with_query_flag'] == with_query_flag) & \n",
    "                                (result_dataframe['evaluation_distance_function'] == distance_function) & (result_dataframe['algorithm_distance_function'] == distance_function) & (~result_dataframe['algorithm'].isin(exclude_algs))].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plot_grouped_barchart(subset_df, 'algorithm', metric, 'query_name', annotate_query_tables= True)\n",
    "#plt.ylim(0,1)\n",
    "#plt.xlabel('Query')\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.tick_params(labelbottom = False)\n",
    "\n",
    "plt.ylabel('Diversity Score')\n",
    "plt_name = \"santos_avg_diversity_cosine\"\n",
    "#plt.title(f'{metric} using {distance_function.upper()} distance in {benchmark_name.replace(\"_\", \" \")}')\n",
    "#plt.title(f'Max-min Diversity Score using Cosine distance in UGEN-V1 Benchmark')\n",
    "# plt.savefig(f\"div_plots/paper_plots/{plt_name}.pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_count = {}\n",
    "for idx, rows in subset_df.iterrows():\n",
    "    alg = rows[\"algorithm\"]\n",
    "    query = rows[\"query_name\"]\n",
    "    div_score = rows[metric]\n",
    "    if query not in highest_count:\n",
    "        highest_count[query] = {alg: div_score}\n",
    "    else:\n",
    "        current_counts = highest_count[query]\n",
    "        if alg in current_counts:\n",
    "            print(\"oops\")\n",
    "        else:\n",
    "            current_counts[alg] = div_score\n",
    "        highest_count[query] = current_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_count = {key: value for key, value in highest_count.items() if len(value) >= 2}\n",
    "highest_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(highest_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_keys = list({outer_key: max(inner_dict, key=inner_dict.get) for outer_key, inner_dict in highest_count.items()}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(max_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_counts = {}\n",
    "second_best_counts = {}\n",
    "\n",
    "for table, methods in highest_count.items():\n",
    "    best_method = max(methods, key=methods.get)\n",
    "    sorted_methods = sorted(methods, key=methods.get, reverse=True)\n",
    "    second_best = sorted_methods[1]  # Second highest score, as the list is sorted in descending order\n",
    "\n",
    "    if best_method not in best_counts:\n",
    "        best_counts[best_method] = 1\n",
    "    else:\n",
    "        best_counts[best_method] += 1\n",
    "\n",
    "    if best_method not in second_best_counts:\n",
    "        second_best_counts[best_method] = {}\n",
    "\n",
    "    if second_best not in second_best_counts[best_method]:\n",
    "        second_best_counts[best_method][second_best] = 1\n",
    "    else:\n",
    "        second_best_counts[best_method][second_best] += 1\n",
    "\n",
    "print(\"Best method counts for each table:\", best_counts)\n",
    "print(\"Second best method counts for each best method:\", second_best_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Load CSV data into a DataFrame\n",
    "x_axis = \"s\"\n",
    "df = pd.read_csv(f'div_stats/efficiency_benchmark_{x_axis}_vs_time_cosine__dust.csv')\n",
    "df['algorithm'] = df['algorithm'].replace({'gmc': r'$GMC$', 'gne': r'$GNE$', \"clt\": r'$CLT$', \"our\": r'$DUST$', \"our_base\": r'$DUST_{c}$'})\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Define a list of color-blind-friendly line styles and markers\n",
    "line_styles = ['-', '--', '-.', ':']\n",
    "markers = ['o', '^', 's', 'D']\n",
    "colors = ['tab:blue', 'tab:red', 'tab:orange', 'tab:green']\n",
    "\n",
    "# Create a dictionary to map algorithms to line styles and markers\n",
    "algorithm_styles = {}\n",
    "for i, algorithm in enumerate(df['algorithm'].unique()):\n",
    "    algorithm_styles[algorithm] = {'line_style': line_styles[i % len(line_styles)],\n",
    "                                   'marker': markers[i % len(markers)], 'color': colors[i % len(colors)]}\n",
    "if x_axis == \"s\":\n",
    "    x_axis = \"|S|\"\n",
    "# Iterate over unique algorithms in the DataFrame and plot each one\n",
    "for algorithm in df['algorithm'].unique():\n",
    "    algorithm_data = df[df['algorithm'] == algorithm]\n",
    "    style = algorithm_styles[algorithm]\n",
    "    ax.plot(algorithm_data[x_axis], algorithm_data['time_taken_(s)'], label=algorithm, linestyle=style['line_style'], marker=style['marker'], color=style['color'])\n",
    "\n",
    "if x_axis == \"|S|\":\n",
    "    x_axis = \"s\"\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.0f}K'.format(x / 1000) if x >= 1000 else '{:.1f}'.format(x)))\n",
    "    ax.set_xlabel('Number of input unionable tuples (s)')\n",
    "else:\n",
    "    ax.set_xlabel('Number of output tuples (k)')\n",
    "\n",
    "#ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.0f}K'.format(x / 1000) if x >= 1000 else '{:.1f}'.format(x)))\n",
    "# Set labels and title\n",
    "\n",
    "ax.set_ylabel('Runtime (seconds)')\n",
    "# ax.set_title('Algorithm Performance Comparison')\n",
    "\n",
    "# Display legend\n",
    "#ax.legend()\n",
    "# ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=len(df['algorithm'].unique()))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tight_layout()\n",
    "plt_name = f\"diversity_{x_axis}_vs_time\"\n",
    "plt.savefig(f\"div_plots/paper_plots/{plt_name}.pdf\", format='pdf', bbox_inches='tight')\n",
    "# Show the plot\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"/home/khatiwada/dust/diversity_algorithms/final_stats/labeled_benchmark__cosine__dust__ourunpruned.csv\")\n",
    "# df = pd.read_csv(r\"/home/khatiwada/dust/diversity_algorithms/final_stats/labeled_benchmark__cosine__dust__filtered_file.csv\")\n",
    "filtered_df = df[df['algorithm'] == 'our']\n",
    "average_time_taken = filtered_df['time_taken_(s)'].mean()\n",
    "average_time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
