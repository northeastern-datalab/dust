{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, random\n",
    "import json, sys, os\n",
    "from pathlib import Path\n",
    "import div_utilities as div_utl\n",
    "import utilities as utl\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizerFast, RobertaModel\n",
    "from model_classes import BertClassifierPretrained, BertClassifier\n",
    "from glove_embeddings import GloveTransformer\n",
    "import fasttext_embeddings as ft\n",
    "from torch.nn.parallel import DataParallel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "embedding_type = \"dust\"\n",
    "# device = \"cpu\"\n",
    "print(\"Model type: \", embedding_type)\n",
    "if embedding_type == \"bert\":\n",
    "    model = BertModel.from_pretrained('bert-base-uncased') \n",
    "    model = BertClassifierPretrained(model).to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    vec_length = 768\n",
    "elif embedding_type == \"roberta\":\n",
    "    model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "    model = BertClassifierPretrained(model).to(device)\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "    vec_length = 768\n",
    "elif embedding_type == \"sentence_bert\":\n",
    "    model = SentenceTransformer('bert-base-uncased').to(device) #case insensitive model. BOSTON and boston have the same embedding.\n",
    "    tokenizer = \"\"\n",
    "    vec_length = 768\n",
    "elif embedding_type == \"glove\":\n",
    "    model = GloveTransformer()\n",
    "    tokenizer = \"\"\n",
    "    vec_length = 300\n",
    "elif embedding_type == \"fasttext\":\n",
    "    model = ft.get_embedding_model()\n",
    "    tokenizer = \"\"\n",
    "    vec_length = 300\n",
    "elif embedding_type == \"dust\":\n",
    "    model_path = r'../out_model/tus_finetune_roberta/checkpoints/best-checkpoint.pt'\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "    model = RobertaModel.from_pretrained('roberta-base')\n",
    "    model = BertClassifier(model, num_labels = 2, hidden_size = 768, output_size = 768)\n",
    "    model = DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "    #print(model)   \n",
    "    model.load_state_dict(torch.load(model_path)) # .to(device)\n",
    "else:\n",
    "    print(\"invalid embedding type\")\n",
    "    sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  58%|█████▊    | 885/1530 [1:57:26<2:58:01, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.51 GiB. GPU 1 has a total capacity of 7.94 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 6.85 GiB memory in use. Of the allocated memory 3.31 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c17_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  58%|█████▊    | 888/1530 [1:58:33<3:10:08, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.52 GiB. GPU 1 has a total capacity of 7.94 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 6.85 GiB memory in use. Of the allocated memory 3.33 GiB is allocated by PyTorch, and 3.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c17_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  58%|█████▊    | 890/1530 [1:59:18<3:17:49, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.40 GiB. GPU 1 has a total capacity of 7.94 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 6.85 GiB memory in use. Of the allocated memory 3.16 GiB is allocated by PyTorch, and 3.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c17_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  59%|█████▊    | 898/1530 [2:01:48<2:43:58, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 1 has a total capacity of 7.94 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 6.85 GiB memory in use. Of the allocated memory 3.34 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c18_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  59%|█████▉    | 900/1530 [2:02:33<3:03:58, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.41 GiB. GPU 1 has a total capacity of 7.94 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 6.85 GiB memory in use. Of the allocated memory 3.17 GiB is allocated by PyTorch, and 3.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c18_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  59%|█████▉    | 905/1530 [2:04:23<2:57:53, 17.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.33 GiB. GPU 1 has a total capacity of 7.94 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 6.85 GiB memory in use. Of the allocated memory 3.05 GiB is allocated by PyTorch, and 3.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c19_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  59%|█████▉    | 908/1530 [2:05:43<3:28:03, 20.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.65 GiB. GPU 0 has a total capacity of 7.94 GiB of which 376.81 MiB is free. Including non-PyTorch memory, this process has 7.56 GiB memory in use. Of the allocated memory 3.51 GiB is allocated by PyTorch, and 3.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c19_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  59%|█████▉    | 910/1530 [2:06:09<2:43:19, 15.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.60 GiB. GPU 0 has a total capacity of 7.94 GiB of which 376.81 MiB is free. Including non-PyTorch memory, this process has 7.56 GiB memory in use. Of the allocated memory 3.43 GiB is allocated by PyTorch, and 3.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c19_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  60%|██████    | 921/1530 [2:06:26<26:31,  2.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.26 GiB. GPU 1 has a total capacity of 7.94 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 6.77 GiB memory in use. Of the allocated memory 2.95 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c20_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  60%|██████    | 922/1530 [2:06:49<1:28:48,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.44 GiB. GPU 1 has a total capacity of 7.94 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 6.77 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c20_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  60%|██████    | 923/1530 [2:07:16<2:21:29, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.82 GiB. GPU 0 has a total capacity of 7.94 GiB of which 16.81 MiB is free. Including non-PyTorch memory, this process has 7.92 GiB memory in use. Of the allocated memory 3.75 GiB is allocated by PyTorch, and 3.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c20_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  60%|██████    | 925/1530 [2:07:44<2:14:08, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 7.94 GiB of which 16.81 MiB is free. Including non-PyTorch memory, this process has 7.92 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 3.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c20_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  61%|██████    | 928/1530 [2:08:38<2:30:03, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacity of 7.94 GiB of which 16.81 MiB is free. Including non-PyTorch memory, this process has 7.92 GiB memory in use. Of the allocated memory 3.67 GiB is allocated by PyTorch, and 3.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c20_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  61%|██████    | 930/1530 [2:09:07<2:22:22, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.67 GiB. GPU 0 has a total capacity of 7.94 GiB of which 16.81 MiB is free. Including non-PyTorch memory, this process has 7.92 GiB memory in use. Of the allocated memory 3.54 GiB is allocated by PyTorch, and 3.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c20_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  61%|██████    | 933/1530 [2:09:58<2:22:31, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 7.94 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 6.16 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 2.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c21_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  61%|██████    | 935/1530 [2:10:34<2:28:51, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.77 GiB. GPU 0 has a total capacity of 7.94 GiB of which 4.81 MiB is free. Including non-PyTorch memory, this process has 7.93 GiB memory in use. Of the allocated memory 3.68 GiB is allocated by PyTorch, and 3.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_93f3d6f7fc6aa6ff____c21_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  67%|██████▋   | 1021/1530 [2:21:45<1:19:02,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 260, in forward\n",
      "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 4.88 GiB is allocated by PyTorch, and 368.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c10_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  67%|██████▋   | 1022/1530 [2:21:47<58:38,  6.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c10_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  67%|██████▋   | 1023/1530 [2:21:48<44:22,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c10_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  67%|██████▋   | 1024/1530 [2:21:49<34:25,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c10_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  67%|██████▋   | 1025/1530 [2:21:51<27:29,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c10_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  68%|██████▊   | 1046/1530 [2:26:04<1:18:10,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  68%|██████▊   | 1047/1530 [2:26:06<58:15,  7.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  68%|██████▊   | 1048/1530 [2:26:07<44:02,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▊   | 1049/1530 [2:26:08<34:06,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▊   | 1050/1530 [2:26:10<27:09,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▊   | 1051/1530 [2:26:11<22:16,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1052/1530 [2:26:13<18:53,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1053/1530 [2:26:14<16:29,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1054/1530 [2:26:15<14:50,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1055/1530 [2:26:17<13:41,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c13_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1056/1530 [2:26:18<12:52,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c14_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1057/1530 [2:26:20<12:18,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c14_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1058/1530 [2:26:21<11:54,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c14_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1059/1530 [2:26:22<11:37,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c14_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  69%|██████▉   | 1060/1530 [2:26:24<11:27,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c14_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  70%|██████▉   | 1066/1530 [2:27:38<1:09:41,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c15_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  70%|██████▉   | 1067/1530 [2:27:40<51:56,  6.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c15_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  70%|██████▉   | 1068/1530 [2:27:41<39:51,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c15_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  70%|██████▉   | 1069/1530 [2:27:43<31:05,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c15_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  70%|██████▉   | 1070/1530 [2:27:44<24:56,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c15_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1081/1530 [2:30:28<1:26:34, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c16_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1082/1530 [2:30:30<1:03:40,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c16_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1083/1530 [2:30:31<47:37,  6.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c16_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1084/1530 [2:30:32<36:28,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c16_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1085/1530 [2:30:34<28:36,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c16_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1086/1530 [2:30:36<23:42,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1087/1530 [2:30:37<19:42,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1088/1530 [2:30:38<16:50,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1089/1530 [2:30:40<14:54,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████   | 1090/1530 [2:30:41<13:31,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████▏  | 1091/1530 [2:30:43<12:35,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████▏  | 1092/1530 [2:30:44<11:53,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  71%|███████▏  | 1093/1530 [2:30:45<11:25,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  72%|███████▏  | 1094/1530 [2:30:47<11:02,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  72%|███████▏  | 1095/1530 [2:30:48<10:51,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c17_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  73%|███████▎  | 1111/1530 [2:35:47<1:42:20, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c19_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  73%|███████▎  | 1112/1530 [2:35:49<1:14:31, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c19_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  73%|███████▎  | 1113/1530 [2:35:50<55:05,  7.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c19_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  73%|███████▎  | 1114/1530 [2:35:52<41:31,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c19_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  73%|███████▎  | 1115/1530 [2:35:53<31:59,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c19_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  74%|███████▍  | 1136/1530 [2:39:50<1:35:52, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  74%|███████▍  | 1137/1530 [2:39:52<1:09:51, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  74%|███████▍  | 1138/1530 [2:39:53<51:40,  7.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  74%|███████▍  | 1139/1530 [2:39:55<38:58,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 1140/1530 [2:39:56<30:06,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 1141/1530 [2:39:58<23:52,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 1142/1530 [2:39:59<19:31,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 1143/1530 [2:40:01<16:30,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 1144/1530 [2:40:02<14:22,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 1145/1530 [2:40:04<12:53,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c21_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 1146/1530 [2:40:05<11:52,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▍  | 1147/1530 [2:40:07<11:07,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 1148/1530 [2:40:08<10:34,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 1149/1530 [2:40:10<10:12,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 1150/1530 [2:40:11<09:59,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 1151/1530 [2:40:13<09:48,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 1152/1530 [2:40:14<09:39,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 1153/1530 [2:40:16<09:33,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 1154/1530 [2:40:17<09:32,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  75%|███████▌  | 1155/1530 [2:40:19<09:28,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c22_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1156/1530 [2:40:20<09:24,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1157/1530 [2:40:22<09:22,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1158/1530 [2:40:23<09:20,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1159/1530 [2:40:25<09:18,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1160/1530 [2:40:26<09:16,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1161/1530 [2:40:28<09:14,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1162/1530 [2:40:29<09:12,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1163/1530 [2:40:31<09:10,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1164/1530 [2:40:32<09:09,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1165/1530 [2:40:34<09:08,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c23_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▌  | 1166/1530 [2:40:35<09:08,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c24_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▋  | 1167/1530 [2:40:37<09:07,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c24_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▋  | 1168/1530 [2:40:38<09:05,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c24_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▋  | 1169/1530 [2:40:40<09:04,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c24_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  76%|███████▋  | 1170/1530 [2:40:41<09:03,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c24_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  78%|███████▊  | 1186/1530 [2:41:36<19:44,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c3_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  78%|███████▊  | 1187/1530 [2:41:37<16:11,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c3_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  78%|███████▊  | 1188/1530 [2:41:38<13:28,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c3_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  78%|███████▊  | 1189/1530 [2:41:39<11:36,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c3_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  78%|███████▊  | 1190/1530 [2:41:41<10:16,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c3_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  79%|███████▉  | 1211/1530 [2:43:43<25:44,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c6_0____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  79%|███████▉  | 1212/1530 [2:43:44<19:59,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c6_0____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  79%|███████▉  | 1213/1530 [2:43:46<16:25,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c6_0____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  79%|███████▉  | 1214/1530 [2:43:47<13:30,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c6_0____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  79%|███████▉  | 1215/1530 [2:43:48<11:28,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c6_0____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████  | 1236/1530 [2:46:10<28:05,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c8_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████  | 1237/1530 [2:46:11<21:31,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c8_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████  | 1238/1530 [2:46:13<16:55,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c8_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████  | 1239/1530 [2:46:14<13:43,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c8_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████  | 1240/1530 [2:46:15<11:30,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c8_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  81%|████████▏ | 1246/1530 [2:47:09<32:03,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c9_1____0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  82%|████████▏ | 1247/1530 [2:47:10<24:15,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c9_1____1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  82%|████████▏ | 1248/1530 [2:47:12<18:47,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c9_1____2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  82%|████████▏ | 1249/1530 [2:47:13<14:59,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c9_1____3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  82%|████████▏ | 1250/1530 [2:47:14<12:30,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/diversity_algorithms/../model_classes.py\", line 39, in forward\n",
      "    outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 835, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 524, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/khatiwada/dust/env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 236, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.94 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 2.69 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Bad table: t_c701386b7c10b107____c9_1____4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 1530/1530 [3:23:31<00:00,  7.98s/it]  \n"
     ]
    }
   ],
   "source": [
    "csv_dir = \"/home/khatiwada/dust/data/tus_benchmark/datalake/\"\n",
    "sd_stats = \"/home/khatiwada/dust/diversity_algorithms/sd_stats/tus_benchmark/\"\n",
    "all_tuple_dict = {} # (table, row_id) -> np.array(756)\n",
    "per_table_stats = {}\n",
    "per_row_distance_from_mean = {}\n",
    "table_id = 0\n",
    "all_tables = glob.glob(f\"{csv_dir}*\") #Path(csv_dir).glob('*')\n",
    "print(\"Total tables: \", len(all_tables))\n",
    "for fpath in tqdm(all_tables, desc = \"Embedding\"):\n",
    "    with open(fpath, errors = 'backslashreplace') as f:\n",
    "        # print(f\"reading table: {fpath}\")\n",
    "        try:\n",
    "            current_tuple_dict = {}\n",
    "            table = pd.read_csv(f, nrows=1000, header=None, on_bad_lines=\"skip\", keep_default_na=False, dtype=str).replace('', 'nan', regex=True).replace(\"\\n\", '', regex=True).replace(\",\", \"\", regex=True).replace(\"'\", \"\", regex=True).replace('\"', '', regex=True)\n",
    "            serialized_tuples = utl.SerializeTable(table)\n",
    "            for idx, tup in enumerate(serialized_tuples):\n",
    "                current_tuple_dict[(fpath.rsplit(os.sep, 1)[-1], idx)] = tup\n",
    "            S_dict = utl.EmbedTuples(list(current_tuple_dict.values()), model, embedding_type,tokenizer, 1000)\n",
    "            S_dict = dict(zip(list(current_tuple_dict.keys()), S_dict))\n",
    "            for table_rowid in S_dict:\n",
    "                all_tuple_dict[table_rowid] = S_dict[table_rowid]\n",
    "            current_table_array = np.array(list(S_dict.values()))\n",
    "            current_table_mean = np.mean(current_table_array, axis=0)\n",
    "            # current_table_std_devs = np.std(current_table_array, axis=1)\n",
    "            cosine_distances = [cosine(embedding, current_table_mean) for embedding in list(S_dict.values())]\n",
    "            euclidean_distances = [euclidean(embedding, current_table_mean) for embedding in list(S_dict.values())]\n",
    "            std_dev_of_cosine_distances = np.std(cosine_distances)\n",
    "            std_dev_of_euclidean_distances = np.std(euclidean_distances)\n",
    "            per_table_stats[fpath.rsplit(os.sep, 1)[-1]] = {\"mean_embedding\": current_table_mean,\n",
    "                                                             \"cosine_sd\": std_dev_of_cosine_distances,\n",
    "                                                               \"euclidean_sd\": std_dev_of_euclidean_distances}\n",
    "            for idx, table_rowid in enumerate(list(S_dict.keys())):\n",
    "                per_row_distance_from_mean[table_rowid] = {\"cosine\": cosine_distances[idx],\n",
    "                                                            \"euclidean\": euclidean_distances[idx]}\n",
    "            table_id += 1\n",
    "            if table_id % 50 == 0:\n",
    "                utl.saveDictionaryAsPickleFile(per_table_stats, sd_stats + \"per_table_stats.pickle\")\n",
    "                utl.saveDictionaryAsPickleFile(per_row_distance_from_mean, sd_stats + \"per_row_distance_from_mean.pickle\")\n",
    "                utl.saveDictionaryAsPickleFile(all_tuple_dict, sd_stats + \"all_tuple_dict.pickle\")\n",
    "           \n",
    "        \n",
    "        except Exception as e:\n",
    "             print(e)\n",
    "             print(\"Bad table:\", fpath.rsplit(os.sep, 1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "utl.saveDictionaryAsPickleFile(per_table_stats, sd_stats + \"per_table_stats.pickle\")\n",
    "utl.saveDictionaryAsPickleFile(per_row_distance_from_mean, sd_stats + \"per_row_distance_from_mean.pickle\")\n",
    "utl.saveDictionaryAsPickleFile(all_tuple_dict, sd_stats + \"all_tuple_dict.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
