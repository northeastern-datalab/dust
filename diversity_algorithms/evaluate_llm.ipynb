{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, sys, os\n",
    "import json, torch, random\n",
    "import numpy as np\n",
    "sys.path.append(\"../\")\n",
    "import utilities as utl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pympler import asizeof\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from bkmeans import BKMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import div_utilities as div_utl\n",
    "import copy\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizerFast, RobertaModel\n",
    "from model_classes import BertClassifierPretrained, BertClassifier\n",
    "from glove_embeddings import GloveTransformer\n",
    "import fasttext_embeddings as ft\n",
    "from torch.nn.parallel import DataParallel\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to input a query table and shortlisted k data lake tuples.\n",
    "benchmark_name = r\"ugen_benchmark\"\n",
    "benchmark_folder_path = r\"../data\" + os.sep + benchmark_name\n",
    "algorithm = \"LLM\"\n",
    "llm_output_path = r\"gpt3_generated_tuples\"\n",
    "union_query_folder_path = benchmark_folder_path + os.sep + \"query\" + os.sep \n",
    "llm_result_folder_path = benchmark_folder_path + os.sep + llm_output_path + os.sep \n",
    "if benchmark_name == r\"ugen_benchmark\":\n",
    "    k = 30\n",
    "elif benchmark_name == r\"labeled_benchmark\":\n",
    "    k = 100\n",
    "else:\n",
    "    print(f\"Unknown benchmark: {benchmark_name}\")\n",
    "    sys.exit()\n",
    "lmda = 0.7\n",
    "s_dict_max = 2500\n",
    "q_dict_max = 100\n",
    "metric = \"cosine\" # cosine, l1, l2\n",
    "embedding_type = \"dust\"\n",
    "eplot_folder_path = r\"div_plots\" + os.sep + \"embedding_plots\" + os.sep \n",
    "cplot_folder_path = r\"div_plots\" + os.sep + \"cluster_plots\" + os.sep \n",
    "result_folder_path = r\"div_result_tables\" + os.sep\n",
    "updated_stats_df_path = r\"final_stats\" + os.sep + benchmark_name + \"__\" + metric + \"__\" + embedding_type + \"__llm.csv\"\n",
    "normalize = True\n",
    "max_metric = False\n",
    "compute_metric = True\n",
    "full_dust = False\n",
    "query_tables = glob.glob(union_query_folder_path + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# diversity results for starmie\n",
    "model_path = r'../out_model/tus_benchmark_corrected_roberta/checkpoints/best-checkpoint.pt'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model = BertClassifier(model, num_labels = 2, hidden_size = 768, output_size = 768)\n",
    "model = DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "#print(model)   \n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats_df = pd.DataFrame(columns = [\"algorithm\", \"embedding_type\", \"query_name\", \"|S|\", \"|q|\", \"k\", \"algorithm_distance_function\", \"evaluation_distance_function\", \"with_query_flag\", \"normalized\", \"max_div_score\", \"max-min_div_score\", \"avg_div_score\", \"time_taken_(s)\"])\n",
    "for query_table in query_tables:\n",
    "    query_name = query_table.rsplit(os.sep,1)[-1]\n",
    "    print(\"Query table name: \", query_name)\n",
    "    tuple_id = 0\n",
    "    dl_tuple_dict = {}\n",
    "    query_table = utl.read_csv_file(union_query_folder_path + query_name)\n",
    "    columns_in_query = set(query_table.columns.astype(str))\n",
    "    columns_in_query = {col.strip() for col in columns_in_query}\n",
    "    unionable_tuples = utl.read_csv_file(llm_result_folder_path + query_name) \n",
    "    unionable_tuples.columns = unionable_tuples.columns.astype(str)\n",
    "    unionable_tuples.columns = unionable_tuples.columns.str.strip()\n",
    "    # print(\"query columns:\", columns_in_query)\n",
    "    before_col_drop = unionable_tuples.copy() \n",
    "    if full_dust == True: \n",
    "        #alignment in dataset is already done in previous phase, we only need to drop the columns not in query.\n",
    "        columns_to_drop = set(unionable_tuples.columns.astype(str)) - columns_in_query\n",
    "        unionable_tuples = unionable_tuples.drop(columns=columns_to_drop)\n",
    "    # print(\"unionable tuples:\", unionable_tuples)               \n",
    "    serialized_tuples = utl.SerializeTable(unionable_tuples)\n",
    "    if len(serialized_tuples) == 0:\n",
    "        # each = {\"metric\": \"l2\", \"with_query\" : \"yes\", \"max_score\": l2_with_query_max_scores, \"max-min_score\": min(l2_with_query_min_scores), \"avg_score\": l2_with_query_avg_scores}\n",
    "        append_list = [algorithm, embedding_type, query_name, len(serialized_tuples), len(query_table), k, metric, \"cosine\", \"mix\", normalize, np.nan, 0, 0, \"n/a\"]\n",
    "        stats_df.loc[len(stats_df)] = append_list\n",
    "        continue\n",
    "    # print(\"serialized_tuples: \", serialized_tuples)    \n",
    "    for tup in serialized_tuples:\n",
    "        dl_tuple_dict[tuple_id] = tup\n",
    "        tuple_id += 1\n",
    "        if len(dl_tuple_dict) >= k:\n",
    "            break\n",
    "    # print(\"Dl tuple dict:\", dl_tuple_dict)\n",
    "    # if len(dl_tuple_dict == 0) :\n",
    "    #     append_list = [algorithm, embedding_type, query_name, len(S_dict), len(q_dict), k, metric, each['metric'], each[\"with_query\"], normalize, each[\"max_score\"], each[\"max-min_score\"], each[\"avg_score\"], \"n/a\"]\n",
    "\n",
    "    S_dict = utl.EmbedTuples(list(dl_tuple_dict.values()), model, embedding_type,tokenizer, 1000)\n",
    "    S_dict = dict(zip(list(dl_tuple_dict.keys()), S_dict))\n",
    "    print(\"Total data lake tuples:\", len(dl_tuple_dict))\n",
    "    query_tuple_dict = {}\n",
    "    serialized_tuples = utl.SerializeTable(query_table)\n",
    "    for tup in serialized_tuples:\n",
    "        query_tuple_dict[tuple_id] = tup\n",
    "        tuple_id += 1\n",
    "    if len(query_tuple_dict) > q_dict_max:\n",
    "        random.seed(random_seed)\n",
    "        sampled_keys = random.sample(query_tuple_dict.keys(), q_dict_max)\n",
    "        sampled_dict = {key: query_tuple_dict[key] for key in sampled_keys}\n",
    "        query_tuple_dict = sampled_dict \n",
    "    q_dict = utl.EmbedTuples(list(query_tuple_dict.values()), model, embedding_type,tokenizer, 1000)\n",
    "    q_dict = dict(zip(list(query_tuple_dict.keys()), q_dict))\n",
    "    print(\"Total query tuples:\", len(query_tuple_dict))\n",
    "    if len(q_dict) < 3:\n",
    "        print(f\"Query table: {query_name} has only {len(q_dict)} rows. So, ignoring this table.\")\n",
    "        continue\n",
    "    computed_metrics, embedding_plot = div_utl.compute_metrics(set(S_dict.keys()), S_dict, q_dict, lmda, k, print_results = False, normalize=normalize, metric=metric, max_metric = max_metric)\n",
    "    # columns = [\"algorithm\", \"embedding_type\", \"query_name\", \"|S|\", \"|q|\", \"k\", \"algorithm_distance_function\", \"evaluation_distance_function\", \"with_query_flag\", \"normalized\", \"max_div_score\", \"max-min_div_score\", \"avg_div_score\", \"time_taken_(s)\"]\n",
    "    for each in computed_metrics:\n",
    "        # each = {\"metric\": \"l2\", \"with_query\" : \"yes\", \"max_score\": l2_with_query_max_scores, \"max-min_score\": min(l2_with_query_min_scores), \"avg_score\": l2_with_query_avg_scores}\n",
    "        append_list = [algorithm, embedding_type, query_name, len(S_dict), len(q_dict), k, metric, each['metric'], each[\"with_query\"], normalize, each[\"max_score\"], each[\"max-min_score\"], each[\"avg_score\"], \"n/a\"]\n",
    "        stats_df.loc[len(stats_df)] = append_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.to_csv(updated_stats_df_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
