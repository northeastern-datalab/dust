{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as p\n",
    "import _pickle as cPickle\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadDictionaryFromPickleFile(dictionaryPath):\n",
    "    ''' Load the pickle file as a dictionary\n",
    "    Args:\n",
    "        dictionaryPath: path to the pickle file\n",
    "    Return: dictionary from the pickle file\n",
    "    '''\n",
    "    filePointer=open(dictionaryPath, 'rb')\n",
    "    dictionary = p.load(filePointer)\n",
    "    filePointer.close()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = loadDictionaryFromPickleFile(r\"/home/khatiwada/starmie/results/ugen_tuple_benchmark_top-100_results_by_starmie.pickle\")\n",
    "results = loadDictionaryFromPickleFile(r\"/home/khatiwada/dust/diversity_algorithms/starmie_results/imdb_case_study_benchmark_top-21_results_by_starmie.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function saves dictionaries as pickle files in the storage.\n",
    "def saveDictionaryAsPickleFile(dictionary, dictionaryPath):\n",
    "    if dictionaryPath.rsplit(\".\")[-1] == \"pickle\":\n",
    "        filePointer=open(dictionaryPath, 'wb')\n",
    "        p.dump(dictionary,filePointer, protocol=p.HIGHEST_PROTOCOL)\n",
    "        filePointer.close()\n",
    "    else: #pbz2 format\n",
    "        with bz2.BZ2File(dictionaryPath, \"w\") as f: \n",
    "            cPickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDictionaryAsPickleFile(results, r\"/home/khatiwada/dust/groundtruth/imdb_case_study_union_groundtruth.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starmie generating the unioned table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder containing the tables\n",
    "folder_path = \"/home/khatiwada/starmie/data/imdb_case_study/datalake\"  # Update with your folder path\n",
    "\n",
    "# Define the list of tables in order\n",
    "union_order = results['query_table.csv']\n",
    "\n",
    "# Read and concatenate the tables\n",
    "unioned_df = pd.concat([pd.read_csv(os.path.join(folder_path, file)) for file in union_order], ignore_index=True)\n",
    "\n",
    "# Save the final unioned table\n",
    "unioned_df.to_csv(os.path.join(folder_path, \"imdb_case_study_starmie_unioned_table.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generating dust unioned table in proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/home/khatiwada/dust/diversity_algorithms/div_result_tables/imdb_case_study/cosine/dust/our/query_table.txt\"  # Change this to your actual file path\n",
    "output_file = \"/home/khatiwada/dust/diversity_algorithms/div_result_tables/imdb_case_study/cosine/dust/our/imdb_case_study_dust_unioned_table.csv\"\n",
    "\n",
    "# Define the list of headers in the specified order\n",
    "headers = [\"Title\", \"Duration\", \"MPA\", \"Rating\", \"grossWorldWide\", \"directors\", \"writers\", \"stars\", \"genres\", \"countries_origin\", \"filming_locations\", \"Languages\", \"release_date\"]\n",
    "\n",
    "# Read the file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse the data\n",
    "data = []\n",
    "current_entry = {}\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    # Split by [SEP] and remove [CLS]\n",
    "    parts = re.split(r\" \\[SEP] \", line.replace(\"[CLS] \", \"\"))\n",
    "    \n",
    "    for part in parts:\n",
    "        key_value = part.split(\"  \", 1)  # Split on double spaces\n",
    "        if len(key_value) == 2:\n",
    "            key, value = key_value\n",
    "            current_entry[key] = value\n",
    "    \n",
    "    if current_entry:\n",
    "        data.append(current_entry)\n",
    "        current_entry = {}\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"CSV file saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Function to count unique values dynamically and plot\n",
    "\n",
    "# Function to count unique values dynamically and plot with steps for two CSV files\n",
    "def plot_unique_counts(csv_file1, csv_file2, max_n_rows, skip_rows1, skip_rows2, step):\n",
    "    df1 = pd.read_csv(csv_file1, skiprows=range(1, skip_rows1+1))\n",
    "    df2 = pd.read_csv(csv_file2, skiprows=range(1, skip_rows2+1))\n",
    "    selected_columns = [\"Title\", \"Languages\", \"filming_locations\"]\n",
    "    for column in selected_columns:\n",
    "        unique_counts1 = []\n",
    "        unique_counts2 = []\n",
    "        x_values = list(range(1, max_n_rows + 1, step))\n",
    "        \n",
    "        for n in x_values:\n",
    "            unique_counts1.append(df1.iloc[:n][column].nunique())\n",
    "            unique_counts2.append(df2.iloc[:n][column].nunique())\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(x_values, unique_counts1, marker='o', linestyle='--', label='Starmie')\n",
    "        plt.plot(x_values, unique_counts2, marker='s', linestyle='-', label='DUST')\n",
    "        plt.xlabel(\"Number of output tuples (k)\")\n",
    "        plt.ylabel(\"Unique Values Count\")\n",
    "        plt.ylim(0, 300)  # Set Y-axis limit from 0 to 500\n",
    "        plt.title(f\"Unique Values in {column} Column\")\n",
    "        # plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3)\n",
    "        # plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "csv_input_file1 = \"/home/khatiwada/dust/diversity_algorithms/starmie_results/imdb_case_study_starmie_unioned_table.csv\"  # Change this to your actual CSV file\n",
    "max_n_rows = 500  # Change this to the desired number of rows to consider\n",
    "skip_rows1 = 0\n",
    "csv_input_file2 = \"/home/khatiwada/dust/diversity_algorithms/div_result_tables/imdb_case_study/cosine/dust/our/imdb_case_study_dust_unioned_table.csv\"  # Change this to your actual CSV file\n",
    "skip_rows2 = 0  # Change this to the number of initial rows to skip\n",
    "step = 50  # Change this to the step size for plotting\n",
    "plot_unique_counts(csv_input_file1, csv_input_file2, max_n_rows, skip_rows1, skip_rows2, step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Function to count unique values dynamically and plot\n",
    "\n",
    "# Function to count unique values dynamically and plot with steps for two CSV files\n",
    "def plot_unique_counts(csv_file1, csv_file2, max_n_rows, skip_rows1, skip_rows2, step):\n",
    "    df1 = pd.read_csv(csv_file1, skiprows=range(1, skip_rows1+1))\n",
    "    df2 = pd.read_csv(csv_file2, skiprows=range(1, skip_rows2+1))\n",
    "    selected_columns = [\"Title\", \"Languages\", \"filming_locations\"]\n",
    "    line_styles = {\"Title\": \"-\", \"Languages\": \"--\", \"filming_locations\": \":\"}\n",
    "    colors = {\"Starmie\": \"blue\", \"DUST\": \"red\"}\n",
    "    markers = {\"Starmie\": \"o\", \"DUST\": \"s\"}\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for column in selected_columns:\n",
    "        unique_counts1 = []\n",
    "        unique_counts2 = []\n",
    "        x_values = list(range(1, max_n_rows + 1, step))\n",
    "        \n",
    "        for n in x_values:\n",
    "            unique_counts1.append(df1.iloc[:n][column].nunique())\n",
    "            unique_counts2.append(df2.iloc[:n][column].nunique())\n",
    "        \n",
    "        plt.plot(x_values, unique_counts1, marker=markers[\"Starmie\"], linestyle=line_styles[column], color=colors[\"Starmie\"], label=f'Starmie - {column}')\n",
    "        plt.plot(x_values, unique_counts2, marker=markers[\"DUST\"], linestyle=line_styles[column], color=colors[\"DUST\"], label=f'DUST - {column}')\n",
    "    \n",
    "   \n",
    "    plt.xlabel(\"Number of output tuples (k)\")\n",
    "    plt.ylabel(\"Unique Values Count\")\n",
    "    plt.ylim(0, 300)  # Set Y-axis limit from 0 to 500\n",
    "    plt.title(f\"Unique Values in {column} Column\")\n",
    "    # plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "csv_input_file1 = \"/home/khatiwada/dust/diversity_algorithms/starmie_results/imdb_case_study_starmie_unioned_table.csv\"  # Change this to your actual CSV file\n",
    "max_n_rows = 500  # Change this to the desired number of rows to consider\n",
    "skip_rows1 = 0\n",
    "csv_input_file2 = \"/home/khatiwada/dust/diversity_algorithms/div_result_tables/imdb_case_study/cosine/dust/our/query_table_formatted.csv\"  # Change this to your actual CSV file\n",
    "skip_rows2 = 0  # Change this to the number of initial rows to skip\n",
    "step = 50  # Change this to the step size for plotting\n",
    "plot_unique_counts(csv_input_file1, csv_input_file2, max_n_rows, skip_rows1, skip_rows2, step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrames\n",
    "df1 = pd.read_csv(r\"/home/khatiwada/dust/data/imdb_case_study/query/query_table.csv\")\n",
    "df2 = pd.read_csv(r\"/home/khatiwada/dust/diversity_algorithms/div_result_tables/imdb_case_study/cosine/dust/our/query_table_formatted.csv\")\n",
    "df2 = pd.read_csv(r\"/home/khatiwada/dust/diversity_algorithms/starmie_results/imdb_case_study_starmie_unioned_table.csv\")\n",
    "overlapping_rows = df1.merge(df2, how='inner').drop_duplicates()\n",
    "\n",
    "# Count number of overlapping rows\n",
    "num_overlapping = len(overlapping_rows)\n",
    "\n",
    "print(\"Number of overlapping rows:\", num_overlapping)\n",
    "print(overlapping_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to count unique values dynamically and plot with steps for two CSV files\n",
    "def plot_unique_counts(csv_file1, csv_file2, query_csv, max_n_rows, skip_rows1, skip_rows2, step):\n",
    "    df1 = pd.read_csv(csv_file1, skiprows=range(1, skip_rows1+1))\n",
    "    df2 = pd.read_csv(csv_file2, skiprows=range(1, skip_rows2+1))\n",
    "    df1_d = pd.read_csv(csv_file1, skiprows=range(1, 99))  # Starmie-D omitting first 98 rows\n",
    "    df_query = pd.read_csv(query_csv)\n",
    "    \n",
    "    selected_columns = [\"Title\", \"Languages\", \"filming_locations\"]\n",
    "    \n",
    "    for column in selected_columns:\n",
    "        unique_counts1 = []\n",
    "        unique_counts2 = []\n",
    "        unique_counts1_d = []\n",
    "        x_values = list(range(1, max_n_rows + 1, step))\n",
    "        query_values = set(df_query[column].dropna().unique())\n",
    "        \n",
    "        for n in x_values:\n",
    "            unique_values1 = set(df1.iloc[:n][column].dropna().unique()) - query_values\n",
    "            unique_values2 = set(df2.iloc[:n][column].dropna().unique()) - query_values\n",
    "            unique_values1_d = set(df1_d.iloc[:n][column].dropna().unique()) - query_values\n",
    "            unique_counts1.append(len(unique_values1))\n",
    "            unique_counts2.append(len(unique_values2))\n",
    "            unique_counts1_d.append(len(unique_values1_d))\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(x_values, unique_counts1, marker='o', color=\"blue\", linestyle='--', label='Starmie')\n",
    "        plt.plot(x_values, unique_counts1_d, marker='^', color=\"red\", linestyle='-.', label='Starmie-D')\n",
    "        plt.plot(x_values, unique_counts2, marker='s', color=\"orange\", linestyle='-', label='DUST')\n",
    "        plt.xlabel(\"Number of output tuples (k)\")\n",
    "        plt.ylabel(\"Unique Values Count\")\n",
    "        plt.ylim(0, 300)  # Set Y-axis limit from 0 to 300\n",
    "        # plt.title(f\"Unique Values in {column} Column\")\n",
    "        # plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "csv_input_file1 = \"/home/khatiwada/dust/diversity_algorithms/starmie_results/imdb_case_study_starmie_unioned_table.csv\"  # Change this to your actual CSV file\n",
    "csv_input_file2 = \"/home/khatiwada/dust/diversity_algorithms/div_result_tables/imdb_case_study/cosine/dust/our/query_table_formatted.csv\"  # Change this to your actual CSV file\n",
    "query_csv = \"/home/khatiwada/dust/data/imdb_case_study/query/query_table.csv\"  # Change this to your query table CSV file\n",
    "max_n_rows = 500  # Change this to the desired number of rows to consider\n",
    "skip_rows1 = 0\n",
    "skip_rows2 = 0  # Change this to the number of initial rows to skip\n",
    "step = 50  # Change this to the step size for plotting\n",
    "plot_unique_counts(csv_input_file1, csv_input_file2, query_csv, max_n_rows, skip_rows1, skip_rows2, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
