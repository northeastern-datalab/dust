{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, sys, os\n",
    "import json, torch, random\n",
    "import numpy as np\n",
    "sys.path.append(\"../\")\n",
    "import utilities as utl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pympler import asizeof\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from bkmeans import BKMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import div_utilities as div_utl\n",
    "import copy\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizerFast, RobertaModel\n",
    "from model_classes import BertClassifierPretrained, BertClassifier\n",
    "from glove_embeddings import GloveTransformer\n",
    "import fasttext_embeddings as ft\n",
    "from torch.nn.parallel import DataParallel\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to input a query table and shortlisted k data lake tuples.\n",
    "benchmark_name = r\"ugen_benchmark\"\n",
    "benchmark_folder_path = r\"/home/khatiwada/starmie/data/ugen_tuple\"\n",
    "algorithm = \"Starmie\"\n",
    "union_query_folder_path = benchmark_folder_path + os.sep + \"query\" + os.sep \n",
    "union_datalake_folder_path = benchmark_folder_path + os.sep + \"datalake\" + os.sep \n",
    "if benchmark_name == r\"ugen_benchmark\":\n",
    "    k = 30\n",
    "    result_size = 100\n",
    "elif benchmark_name == r\"labeled_benchmark\":\n",
    "    k = 100\n",
    "    result_size = 500\n",
    "else:\n",
    "    print(f\"Unknown benchmark: {benchmark_name}\")\n",
    "    sys.exit()\n",
    "lmda = 0.7\n",
    "s_dict_max = 2500\n",
    "q_dict_max = 100\n",
    "metric = \"cosine\" # cosine, l1, l2\n",
    "embedding_type = \"dust\"\n",
    "eplot_folder_path = r\"div_plots\" + os.sep + \"embedding_plots\" + os.sep \n",
    "cplot_folder_path = r\"div_plots\" + os.sep + \"cluster_plots\" + os.sep \n",
    "result_folder_path = r\"div_result_tables\" + os.sep\n",
    "stats_df_path = r\"div_stats\" + os.sep + benchmark_name + \"__\" + metric + \"__\" + embedding_type + \".csv\"\n",
    "updated_stats_df_path = r\"final_stats\" + os.sep + benchmark_name + \"__\" + metric + \"__\" + embedding_type + \"__starmie.csv\"\n",
    "normalize = True\n",
    "max_metric = False\n",
    "compute_metric = True\n",
    "full_dust = False\n",
    "save_results = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_result_path = os.path.join(r\"div_result_tables\", benchmark_name, metric, embedding_type)\n",
    "# Create directory if it does not exist\n",
    "if not os.path.exists(div_result_path):\n",
    "    os.makedirs(div_result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# diversity results for starmie\n",
    "model_path = r'../out_model/tus_benchmark_corrected_roberta/checkpoints/best-checkpoint.pt'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model = BertClassifier(model, num_labels = 2, hidden_size = 768, output_size = 768)\n",
    "model = DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "#print(model)   \n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results by starmie\n",
    "output_tuples = utl.loadDictionaryFromPickleFile(f\"starmie_results/{benchmark_name}_top-{result_size}_results_by_starmie.pickle\")\n",
    "output_tuples_pr = {key: value[:k] for key, value in output_tuples.items()}\n",
    "output_tuples_for_accuracy = {}\n",
    "for key, value in output_tuples_pr.items():\n",
    "    new_value = []\n",
    "    for val in value:\n",
    "        split_val = val.rsplit(\"_\",1)[0]\n",
    "        split_ext = val.rsplit(\".\",1)[-1]\n",
    "        new_value.append(split_val + \".\" + split_ext)\n",
    "    output_tuples_for_accuracy[key] = new_value\n",
    "union_groundtruth_file_path = f\"../groundtruth/{benchmark_name}_union_groundtruth.pickle\"\n",
    "union_groundtruth = utl.loadDictionaryFromPickleFile(union_groundtruth_file_path)\n",
    "union_groundtruth = {key: set(value) for key, value in union_groundtruth.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_query_precision = {}\n",
    "all_query_map = {}\n",
    "for query in output_tuples_for_accuracy:\n",
    "    current_tuples = output_tuples_for_accuracy[query]\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    current_truth = [0 for i in range(0, k)]\n",
    "    for idx, each in enumerate(current_tuples):\n",
    "        if each in union_groundtruth[query]:\n",
    "            true_positives += 1\n",
    "            current_truth[idx] = 1                \n",
    "        else:\n",
    "            false_positives += 1\n",
    "            current_truth[idx] = 0\n",
    "    if true_positives + false_positives == 0:\n",
    "        current_precision = 0\n",
    "    else:\n",
    "        current_precision = true_positives / k\n",
    "    all_query_precision[query] = current_precision\n",
    "    each_precision = []\n",
    "    for k_dash in range(0, k):\n",
    "        current_true_positives = sum(current_truth[:k_dash])\n",
    "        each_precision.append(current_true_positives / (k_dash + 1))\n",
    "    current_map = sum(each_precision) / len(each_precision)\n",
    "    all_query_map[query] = current_map\n",
    "print(\"Average Precision:\", sum(list(all_query_precision.values()))/ len(all_query_precision)) \n",
    "print(\"MAP: \", sum(list(all_query_map.values()))/ len(all_query_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_query_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(columns = [\"algorithm\", \"embedding_type\", \"query_name\", \"|S|\", \"|q|\", \"k\", \"algorithm_distance_function\", \"evaluation_distance_function\", \"with_query_flag\", \"normalized\", \"max_div_score\", \"max-min_div_score\", \"avg_div_score\", \"time_taken_(s)\"])\n",
    "for query_name in output_tuples:\n",
    "    try:\n",
    "        query_table = utl.read_csv_file(union_query_folder_path + query_name)\n",
    "        columns_in_query = set(query_table.columns.astype(str))\n",
    "        # print(\"Current dl columns:\", query_table.columns)\n",
    "        tuple_id = 0\n",
    "        dl_tuple_dict = {}\n",
    "        unionable_tables = output_tuples[query_name]  \n",
    "        for dl_table in unionable_tables:\n",
    "            current_dl_table = utl.read_csv_file(union_datalake_folder_path + dl_table)\n",
    "            current_dl_table.columns = current_dl_table.columns.astype(str)\n",
    "            # print(\"Current dl columns:\", current_dl_table.columns)\n",
    "            if full_dust == True: \n",
    "                #alignment in dataset is already done in previous phase, we only need to drop the columns not present in query.\n",
    "                columns_to_drop = set(current_dl_table.columns.astype(str)) - columns_in_query\n",
    "                current_dl_table = current_dl_table.drop(columns=columns_to_drop)\n",
    "            serialized_tuples = utl.SerializeTable(current_dl_table)\n",
    "            for tup in serialized_tuples:\n",
    "                dl_tuple_dict[tuple_id] = tup\n",
    "                tuple_id += 1\n",
    "                if len(dl_tuple_dict) >= k:\n",
    "                    break\n",
    "            if len(dl_tuple_dict) >= k:\n",
    "                break\n",
    "        \n",
    "        S_dict = utl.EmbedTuples(list(dl_tuple_dict.values()), model, embedding_type,tokenizer, 1000)\n",
    "        S_dict = dict(zip(list(dl_tuple_dict.keys()), S_dict))\n",
    "        print(\"Total data lake tuples:\", len(dl_tuple_dict))\n",
    "        if len(S_dict) < k and len(unionable_tables) >= k:\n",
    "            # the method is not returning tuples to embed\n",
    "                # each = {\"metric\": \"l2\", \"with_query\" : \"yes\", \"max_score\": l2_with_query_max_scores, \"max-min_score\": min(l2_with_query_min_scores), \"avg_score\": l2_with_query_avg_scores}\n",
    "            append_list = [algorithm, embedding_type, query_name, len(serialized_tuples), len(query_table), k, metric, \"cosine\", \"mix\", normalize, np.nan, 0, 0, \"n/a\"]\n",
    "            stats_df.loc[len(stats_df)] = append_list\n",
    "            continue\n",
    "        query_tuple_dict = {}\n",
    "        serialized_tuples = utl.SerializeTable(query_table)\n",
    "        for tup in serialized_tuples:\n",
    "            query_tuple_dict[tuple_id] = tup\n",
    "            tuple_id += 1\n",
    "        if len(query_tuple_dict) > q_dict_max:\n",
    "            random.seed(random_seed)\n",
    "            sampled_keys = random.sample(query_tuple_dict.keys(), q_dict_max)\n",
    "            sampled_dict = {key: query_tuple_dict[key] for key in sampled_keys}\n",
    "            query_tuple_dict = sampled_dict \n",
    "        q_dict = utl.EmbedTuples(list(query_tuple_dict.values()), model, embedding_type,tokenizer, 1000)\n",
    "        q_dict = dict(zip(list(query_tuple_dict.keys()), q_dict))\n",
    "        print(\"Total query tuples:\", len(query_tuple_dict))\n",
    "        if len(q_dict) < 3:\n",
    "            print(f\"Query table: {query_name} has only {len(q_dict)} rows. So, ignoring this table.\")\n",
    "            continue\n",
    "        computed_metrics, embedding_plot = div_utl.compute_metrics(set(S_dict.keys()), S_dict, q_dict, lmda, k, print_results = False, normalize=normalize, metric=metric, max_metric = max_metric)\n",
    "        # columns = [\"algorithm\", \"embedding_type\", \"query_name\", \"|S|\", \"|q|\", \"k\", \"algorithm_distance_function\", \"evaluation_distance_function\", \"with_query_flag\", \"normalized\", \"max_div_score\", \"max-min_div_score\", \"avg_div_score\", \"time_taken_(s)\"]\n",
    "        for each in computed_metrics:\n",
    "            # each = {\"metric\": \"l2\", \"with_query\" : \"yes\", \"max_score\": l2_with_query_max_scores, \"max-min_score\": min(l2_with_query_min_scores), \"avg_score\": l2_with_query_avg_scores}\n",
    "            append_list = [algorithm, embedding_type, query_name, len(S_dict), len(q_dict), k, metric, each['metric'], each[\"with_query\"], normalize, each[\"max_score\"], each[\"max-min_score\"], each[\"avg_score\"], \"n/a\"]\n",
    "            stats_df.loc[len(stats_df)] = append_list\n",
    "        if save_results == True:\n",
    "            f_path = os.path.join(div_result_path, \"starmie\")\n",
    "            if not os.path.exists(f_path):\n",
    "                os.makedirs(f_path)\n",
    "            # print(\"Dl dict:\", dl_tuple_dict)\n",
    "            current_div_results_path = f_path + os.sep + query_name.rsplit(\".\",1)[0] + \".txt\"\n",
    "            with open(current_div_results_path, \"w\") as f:\n",
    "                for i in range(0, len(dl_tuple_dict)):\n",
    "                #div_tuple in dl_tuple_dict:\n",
    "                    f.write(dl_tuple_dict[i] + \"\\n\")\n",
    "    except:\n",
    "        print(\"failed on: \", query_name)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.to_csv(updated_stats_df_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
